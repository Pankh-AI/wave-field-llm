{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave Field LLM V4.3.3 â€” Training & Diagnostics\n",
    "\n",
    "**Physics-based O(n log n) attention vs Standard O(nÂ²) Transformer**\n",
    "\n",
    "This notebook trains both architectures on WikiText-103 with full monitoring:\n",
    "- Live training curves with `tqdm` progress bars\n",
    "- Physics diagnostics at every checkpoint (kernel health, gate activity, rank)\n",
    "- Side-by-side comparison tables via `pandas`\n",
    "- Generation samples at each checkpoint\n",
    "- Speed & memory scaling benchmarks\n",
    "\n",
    "**GPU:** T4 (15.6 GB) â€” supports up to ~120M params with gradient checkpointing\n",
    "\n",
    "**Architecture:** Wave Field V4.3.3 (SPECTRE-Wave) with:\n",
    "- Learned feature maps (Hedgehog, ICLR 2024)\n",
    "- HiPPO kernel init (S4D)\n",
    "- Content-adaptive spectral gate (SPECTRE)\n",
    "- Per-group LR (kernel params at 50x)\n",
    "- `torch.compile` on non-FFT submodules\n",
    "- cuFFT-optimal padding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 1: SETUP â€” Clone repo + install deps\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "REPO = '/content/wave-field-llm'\n",
    "if not os.path.exists(REPO):\n",
    "    !git clone https://github.com/Pankh-AI/wave-field-llm.git {REPO}\n",
    "else:\n",
    "    !cd {REPO} && git pull --ff-only\n",
    "\n",
    "os.chdir(REPO)\n",
    "!pip install -q datasets tokenizers matplotlib tqdm pandas ipywidgets\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(f\"Working dir: {os.getcwd()}\")\n",
    "print(\"Setup complete\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 2: IMPORTS + CONFIG\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time, math, gc, sys, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "from src.wave_field_transformer import WaveFieldTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style='background:#1a1a2e;color:#e0e0e0;padding:15px;border-radius:8px;font-family:monospace'>\n",
    "  <h3 style='color:#00d4ff;margin:0'>Wave Field LLM V4.3.3</h3>\n",
    "  <p>GPU: <b>{gpu_name}</b> | VRAM: <b>{vram_gb:.1f} GB</b> | PyTorch: <b>{torch.__version__}</b></p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "# ============== MODEL CONFIG ==============\n",
    "# Two scale options â€” pick one:\n",
    "\n",
    "SCALE = 'S1'  # 'S1' = ~22M (fast, proof-of-concept) | 'S3' = ~120M (full scale)\n",
    "\n",
    "CONFIGS = {\n",
    "    'S1': dict(embed_dim=384, num_layers=8, num_heads=8, ffn_dim=1536,\n",
    "              field_size=1536, seq_len=512, batch_size=8, grad_accum=4,\n",
    "              peak_lr=6e-4, total_steps=2000, eval_every=200, ckpt_every=500),\n",
    "    'S3': dict(embed_dim=768, num_layers=12, num_heads=12, ffn_dim=3072,\n",
    "              field_size=2048, seq_len=512, batch_size=4, grad_accum=8,\n",
    "              peak_lr=3e-4, total_steps=6000, eval_every=500, ckpt_every=1000),\n",
    "}\n",
    "\n",
    "CFG = CONFIGS[SCALE]\n",
    "BPE_VOCAB = 8192\n",
    "USE_FP16 = True   # T4: fp16 + GradScaler (NOT bf16 â€” T4 bf16 is emulated)\n",
    "CKPT_DIR = Path('checkpoints')\n",
    "\n",
    "print(f\"Scale: {SCALE} | embed={CFG['embed_dim']} layers={CFG['num_layers']} \"\n",
    "      f\"heads={CFG['num_heads']} ffn={CFG['ffn_dim']} field={CFG['field_size']}\")\n",
    "print(f\"Seq: {CFG['seq_len']} | Batch: {CFG['batch_size']} x {CFG['grad_accum']} accum \"\n",
    "      f\"= {CFG['batch_size'] * CFG['grad_accum']} effective\")\n",
    "print(f\"Steps: {CFG['total_steps']} | Eval every: {CFG['eval_every']} | Ckpt every: {CFG['ckpt_every']}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 3: STANDARD TRANSFORMER BASELINE\n",
    "# ============================================================\n",
    "\n",
    "class StandardTransformer(nn.Module):\n",
    "    \"\"\"Standard O(n^2) Transformer for fair comparison.\n",
    "    Same embedding dim, layers, heads, FFN. Uses PyTorch's built-in\n",
    "    TransformerEncoder with pre-norm and GELU.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_layers=6,\n",
    "                 num_heads=8, ffn_dim=1024, max_seq_len=514, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=ffn_dim,\n",
    "            dropout=dropout, activation='gelu', batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        self.output_projection.weight = self.token_embedding.weight\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, 0, 0.02)\n",
    "\n",
    "    def forward(self, input_ids, labels=None, mask=None):\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        B, N = input_ids.shape\n",
    "        pos = torch.arange(N, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
    "        x = self.token_embedding(input_ids) + self.positional_embedding(pos)\n",
    "        x = self.dropout(x)\n",
    "        causal = torch.triu(torch.full((N, N), float('-inf'),\n",
    "                            device=input_ids.device), diagonal=1)\n",
    "        x = self.transformer(x, mask=causal)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size),\n",
    "                                  labels.view(-1), ignore_index=-100)\n",
    "        return logits, loss\n",
    "\n",
    "print(\"StandardTransformer defined\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: DATA â€” WikiText-103 (103M tokens, proper scale)\n",
    "# ============================================================\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "\n",
    "print(\"Loading WikiText-103...\")\n",
    "ds = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "# Filter empty lines and headers\n",
    "train_texts = [t for t in ds['train']['text'] if len(t.strip()) > 100]\n",
    "val_texts = [t for t in ds['validation']['text'] if len(t.strip()) > 100]\n",
    "test_texts = [t for t in ds['test']['text'] if len(t.strip()) > 100]\n",
    "\n",
    "print(f\"  Train docs: {len(train_texts):,} | Val docs: {len(val_texts):,} | Test: {len(test_texts):,}\")\n",
    "print(f\"  Est. train chars: {sum(len(t) for t in train_texts):,}\")\n",
    "\n",
    "# Train BPE tokenizer\n",
    "print(f\"\\nTraining BPE ({BPE_VOCAB} vocab)...\")\n",
    "raw_tok = Tokenizer(models.BPE())\n",
    "raw_tok.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "raw_tok.decoder = decoders.ByteLevel()\n",
    "tok_trainer = trainers.BpeTrainer(\n",
    "    vocab_size=BPE_VOCAB,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
    "    min_frequency=2)\n",
    "raw_tok.train_from_iterator(train_texts[:20000], tok_trainer)\n",
    "VOCAB_SIZE = raw_tok.get_vocab_size()\n",
    "print(f\"  Vocab: {VOCAB_SIZE}\")\n",
    "\n",
    "# Tokenize into flat streams (batch encode â€” 10-20x faster than one-by-one)\n",
    "print(\"Tokenizing (batch mode)...\")\n",
    "BATCH = 4096\n",
    "train_ids = []\n",
    "for i in tqdm(range(0, len(train_texts), BATCH), desc='Train', leave=False):\n",
    "    batch = train_texts[i:i+BATCH]\n",
    "    for enc in raw_tok.encode_batch(batch):\n",
    "        if enc.ids:\n",
    "            train_ids.extend(enc.ids)\n",
    "\n",
    "val_ids = []\n",
    "for i in range(0, len(val_texts), BATCH):\n",
    "    for enc in raw_tok.encode_batch(val_texts[i:i+BATCH]):\n",
    "        if enc.ids:\n",
    "            val_ids.extend(enc.ids)\n",
    "\n",
    "print(f\"  Train tokens: {len(train_ids):,} ({len(train_ids)/1e6:.1f}M)\")\n",
    "print(f\"  Val tokens:   {len(val_ids):,} ({len(val_ids)/1e6:.1f}M)\")\n",
    "\n",
    "# ---- Data utilities ----\n",
    "def make_chunks_gpu(token_ids, seq_len):\n",
    "    \"\"\"Create (input, target) chunk tensors on GPU. Zero CPU->GPU overhead.\"\"\"\n",
    "    all_ids = torch.tensor(token_ids, dtype=torch.long, device=device)\n",
    "    n_chunks = (len(token_ids) - 1) // seq_len\n",
    "    if n_chunks == 0:\n",
    "        return None, None\n",
    "    usable = n_chunks * seq_len\n",
    "    x = all_ids[:usable].reshape(n_chunks, seq_len)\n",
    "    y = all_ids[1:usable + 1].reshape(n_chunks, seq_len)\n",
    "    return x, y\n",
    "\n",
    "def make_batches_gpu(x, y, batch_size, shuffle=True):\n",
    "    \"\"\"Yield batches from GPU-resident tensors.\"\"\"\n",
    "    n = x.shape[0]\n",
    "    if n < batch_size:\n",
    "        return\n",
    "    idx = torch.randperm(n, device=x.device) if shuffle else torch.arange(n, device=x.device)\n",
    "    for s in range(0, n - batch_size + 1, batch_size):\n",
    "        bi = idx[s:s + batch_size]\n",
    "        yield x[bi], y[bi]\n",
    "\n",
    "# Pre-chunk for training\n",
    "seq_len = CFG['seq_len']\n",
    "train_x, train_y = make_chunks_gpu(train_ids, seq_len)\n",
    "val_x, val_y = make_chunks_gpu(val_ids, seq_len)\n",
    "print(f\"\\nGPU-resident chunks (seq={seq_len}):\")\n",
    "print(f\"  Train: {train_x.shape[0]:,} | Val: {val_x.shape[0]:,}\")\n",
    "print(\"Data ready\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: TRAINING ENGINE + PHYSICS MONITOR\n",
    "# ============================================================\n",
    "\n",
    "class WarmupCosineScheduler:\n",
    "    \"\"\"Linear warmup then cosine decay.\"\"\"\n",
    "    def __init__(self, optimizer, warmup, total, min_lr=1e-5):\n",
    "        self.optimizer, self.warmup, self.total = optimizer, warmup, total\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lrs = [pg['lr'] for pg in optimizer.param_groups]\n",
    "        self.step_count = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        for pg, blr in zip(self.optimizer.param_groups, self.base_lrs):\n",
    "            if self.step_count <= self.warmup:\n",
    "                pg['lr'] = blr * (self.step_count / self.warmup)\n",
    "            else:\n",
    "                p = (self.step_count - self.warmup) / max(1, self.total - self.warmup)\n",
    "                pg['lr'] = self.min_lr + 0.5 * (blr - self.min_lr) * (1 + math.cos(math.pi * p))\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [pg['lr'] for pg in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "class PhysicsMonitor:\n",
    "    \"\"\"Lightweight diagnostics for Wave Field models.\n",
    "\n",
    "    Tracks per-checkpoint:\n",
    "    - Kernel params (frequency, damping, phase) per layer/head\n",
    "    - Gate activation statistics\n",
    "    - Output rank (SVD) per layer\n",
    "    - Gradient norms per parameter group\n",
    "    - Feature map activation stats\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def snapshot(self, model, step, sample_x=None):\n",
    "        \"\"\"Capture physics state. Call during eval (model.eval()).\"\"\"\n",
    "        snap = {'step': step, 'layers': []}\n",
    "\n",
    "        for li, layer in enumerate(model.layers):\n",
    "            attn = layer.attention\n",
    "            H = attn.num_heads\n",
    "            layer_data = {'layer': li}\n",
    "\n",
    "            # 1. Kernel parameters\n",
    "            freq = attn.wave_frequency.detach().cpu().numpy()\n",
    "            damp = F.softplus(attn.wave_damping).detach().cpu().numpy()\n",
    "            phase = attn.wave_phase.detach().cpu().numpy()\n",
    "            layer_data['freq'] = freq.tolist()\n",
    "            layer_data['damp'] = damp.tolist()\n",
    "            layer_data['phase'] = phase.tolist()\n",
    "\n",
    "            # 2. Gate bias (should stay ~2.0 for healthy training)\n",
    "            D = attn.embedding_dim\n",
    "            gate_bias = attn.qkvg_proj.bias[3*D:].detach().cpu()\n",
    "            layer_data['gate_bias_mean'] = gate_bias.mean().item()\n",
    "\n",
    "            # 3. Field coupling strength\n",
    "            coupling = F.softmax(attn.field_coupling.detach().cpu(), dim=-1)\n",
    "            off_diag = coupling.clone()\n",
    "            off_diag.fill_diagonal_(0)\n",
    "            layer_data['cross_coupling'] = off_diag.sum().item()\n",
    "\n",
    "            # 4. Gradient norms (if available)\n",
    "            kernel_grad_norm = 0.0\n",
    "            for name in ['wave_frequency', 'wave_damping', 'wave_phase']:\n",
    "                p = getattr(attn, name)\n",
    "                if p.grad is not None:\n",
    "                    kernel_grad_norm += p.grad.norm().item()\n",
    "            layer_data['kernel_grad_norm'] = kernel_grad_norm\n",
    "\n",
    "            snap['layers'].append(layer_data)\n",
    "\n",
    "        # 5. Output rank from a sample forward pass\n",
    "        if sample_x is not None and hasattr(model, 'layers'):\n",
    "            x = model.token_embedding(sample_x[:2])\n",
    "            pos = model.positional_encoding(sample_x.shape[1], sample_x.device)\n",
    "            x = x + pos.unsqueeze(0)\n",
    "            x = model.dropout(x)\n",
    "            for li, layer_mod in enumerate(model.layers):\n",
    "                x = layer_mod(x)\n",
    "                # Rank: number of singular values > 1% of max\n",
    "                s = torch.linalg.svdvals(x[0].float())\n",
    "                rank = (s > s[0] * 0.01).sum().item()\n",
    "                snap['layers'][li]['output_rank'] = rank\n",
    "                snap['layers'][li]['norm_mean'] = x.norm(dim=-1).mean().item()\n",
    "\n",
    "        self.snapshots.append(snap)\n",
    "        return snap\n",
    "\n",
    "    def summary_df(self):\n",
    "        \"\"\"Return a pandas DataFrame of kernel params across snapshots.\"\"\"\n",
    "        rows = []\n",
    "        for snap in self.snapshots:\n",
    "            for ld in snap['layers']:\n",
    "                for h in range(len(ld['freq'])):\n",
    "                    rows.append({\n",
    "                        'step': snap['step'],\n",
    "                        'layer': ld['layer'],\n",
    "                        'head': h,\n",
    "                        'freq': ld['freq'][h],\n",
    "                        'damp': ld['damp'][h],\n",
    "                        'phase': ld['phase'][h],\n",
    "                        'gate_bias': ld.get('gate_bias_mean', 0),\n",
    "                        'cross_coupling': ld.get('cross_coupling', 0),\n",
    "                        'kernel_grad': ld.get('kernel_grad_norm', 0),\n",
    "                        'output_rank': ld.get('output_rank', -1),\n",
    "                        'norm_mean': ld.get('norm_mean', -1),\n",
    "                    })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate(model, val_x, val_y, batch_size, vocab_size):\n",
    "    \"\"\"Evaluate model on val data. Returns (loss, ppl, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_tokens, n = 0, 0, 0, 0\n",
    "    n_val = min(val_x.shape[0], 500)\n",
    "    with torch.no_grad():\n",
    "        for x, y in make_batches_gpu(val_x[:n_val], val_y[:n_val],\n",
    "                                     batch_size, shuffle=False):\n",
    "            with torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "                logits, _ = model(x)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_tokens += y.numel()\n",
    "            n += 1\n",
    "    avg_loss = total_loss / max(n, 1)\n",
    "    ppl = math.exp(min(avg_loss, 20))\n",
    "    acc = total_correct / max(total_tokens, 1) * 100\n",
    "    return avg_loss, ppl, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt=\"The\", max_tokens=80,\n",
    "                  temperature=0.8, top_k=40):\n",
    "    \"\"\"Autoregressive text generation.\"\"\"\n",
    "    model.eval()\n",
    "    ids = tokenizer.encode(prompt).ids\n",
    "    if not ids:\n",
    "        return \"(empty prompt)\"\n",
    "    input_ids = torch.tensor([ids], device=device)\n",
    "    max_ctx = getattr(model, 'max_seq_len', 2048)\n",
    "    if hasattr(model, 'positional_embedding'):\n",
    "        max_ctx = model.positional_embedding.weight.shape[0]\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        ctx = input_ids[:, -max_ctx:] if input_ids.shape[1] > max_ctx else input_ids\n",
    "        with torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            logits, _ = model(ctx)\n",
    "        logits = logits[0, -1] / temperature\n",
    "        if top_k > 0:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[-1]] = float('-inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)\n",
    "    return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "print(\"Training engine ready\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: VISUALIZATION HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def plot_training_curves(wave_history, std_history, title_suffix=''):\n",
    "    \"\"\"Live training curve comparison.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "    # PPL\n",
    "    ax = axes[0]\n",
    "    if wave_history:\n",
    "        ax.plot([h['step'] for h in wave_history],\n",
    "                [h['ppl'] for h in wave_history],\n",
    "                'r-o', label='Wave V4.3.3', linewidth=2, markersize=4)\n",
    "    if std_history:\n",
    "        ax.plot([h['step'] for h in std_history],\n",
    "                [h['ppl'] for h in std_history],\n",
    "                'b-s', label='Standard', linewidth=2, markersize=4)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Val PPL')\n",
    "    ax.set_title(f'Perplexity {title_suffix}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Accuracy\n",
    "    ax = axes[1]\n",
    "    if wave_history:\n",
    "        ax.plot([h['step'] for h in wave_history],\n",
    "                [h['acc'] for h in wave_history],\n",
    "                'r-o', label='Wave', linewidth=2, markersize=4)\n",
    "    if std_history:\n",
    "        ax.plot([h['step'] for h in std_history],\n",
    "                [h['acc'] for h in std_history],\n",
    "                'b-s', label='Standard', linewidth=2, markersize=4)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Next-token Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Tokens/s\n",
    "    ax = axes[2]\n",
    "    if wave_history:\n",
    "        wh = [h for h in wave_history if h.get('tps', 0) > 0]\n",
    "        if wh:\n",
    "            ax.plot([h['step'] for h in wh],\n",
    "                    [h['tps']/1e3 for h in wh],\n",
    "                    'r-o', label='Wave', linewidth=2, markersize=4)\n",
    "    if std_history:\n",
    "        sh = [h for h in std_history if h.get('tps', 0) > 0]\n",
    "        if sh:\n",
    "            ax.plot([h['step'] for h in sh],\n",
    "                    [h['tps']/1e3 for h in sh],\n",
    "                    'b-s', label='Standard', linewidth=2, markersize=4)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('K tok/s')\n",
    "    ax.set_title('Throughput')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_physics_diagnostics(monitor, title='Wave Field Physics'):\n",
    "    \"\"\"Plot kernel evolution, rank, and gradient health.\"\"\"\n",
    "    if not monitor.snapshots:\n",
    "        print(\"No snapshots yet.\")\n",
    "        return\n",
    "\n",
    "    df = monitor.summary_df()\n",
    "    steps = sorted(df['step'].unique())\n",
    "    n_layers = df['layer'].nunique()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 1. Frequency evolution\n",
    "    ax = axes[0, 0]\n",
    "    for layer_idx in [0, n_layers - 1]:\n",
    "        sub = df[df['layer'] == layer_idx]\n",
    "        for h in sub['head'].unique():\n",
    "            hd = sub[sub['head'] == h]\n",
    "            ax.plot(hd['step'], hd['freq'], '-', alpha=0.6,\n",
    "                    label=f'L{layer_idx}H{h}' if h < 3 else None)\n",
    "    ax.set_xlabel('Step'); ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Kernel Frequencies'); ax.legend(fontsize=7, ncol=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Damping evolution\n",
    "    ax = axes[0, 1]\n",
    "    for layer_idx in [0, n_layers - 1]:\n",
    "        sub = df[df['layer'] == layer_idx]\n",
    "        for h in sub['head'].unique():\n",
    "            hd = sub[sub['head'] == h]\n",
    "            ax.plot(hd['step'], hd['damp'], '-', alpha=0.6,\n",
    "                    label=f'L{layer_idx}H{h}' if h < 3 else None)\n",
    "    ax.set_xlabel('Step'); ax.set_ylabel('Damping (softplus)')\n",
    "    ax.set_title('Kernel Damping'); ax.legend(fontsize=7, ncol=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Output rank per layer\n",
    "    ax = axes[0, 2]\n",
    "    rank_df = df[df['output_rank'] >= 0].groupby(['step', 'layer'])['output_rank'].first().reset_index()\n",
    "    if not rank_df.empty:\n",
    "        for li in rank_df['layer'].unique():\n",
    "            sub = rank_df[rank_df['layer'] == li]\n",
    "            ax.plot(sub['step'], sub['output_rank'], '-o', markersize=3, label=f'Layer {li}')\n",
    "    ax.set_xlabel('Step'); ax.set_ylabel('Effective Rank')\n",
    "    ax.set_title('Output Rank (collapse = 1)'); ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Gate bias\n",
    "    ax = axes[1, 0]\n",
    "    gb_df = df.groupby(['step', 'layer'])['gate_bias'].first().reset_index()\n",
    "    for li in gb_df['layer'].unique():\n",
    "        sub = gb_df[gb_df['layer'] == li]\n",
    "        ax.plot(sub['step'], sub['gate_bias'], '-', label=f'L{li}')\n",
    "    ax.axhline(y=2.0, color='gray', linestyle='--', alpha=0.5, label='init=2.0')\n",
    "    ax.set_xlabel('Step'); ax.set_ylabel('Mean Gate Bias')\n",
    "    ax.set_title('Gate Bias Health'); ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Cross-coupling\n",
    "    ax = axes[1, 1]\n",
    "    cc_df = df.groupby(['step', 'layer'])['cross_coupling'].first().reset_index()\n",
    "    for li in cc_df['layer'].unique():\n",
    "        sub = cc_df[cc_df['layer'] == li]\n",
    "        ax.plot(sub['step'], sub['cross_coupling'], '-', label=f'L{li}')\n",
    "    ax.set_xlabel('Step'); ax.set_ylabel('Total Cross-Coupling')\n",
    "    ax.set_title('Field Coupling Activity'); ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Norm growth\n",
    "    ax = axes[1, 2]\n",
    "    nm_df = df[df['norm_mean'] > 0].groupby(['step', 'layer'])['norm_mean'].first().reset_index()\n",
    "    if not nm_df.empty:\n",
    "        for li in nm_df['layer'].unique():\n",
    "            sub = nm_df[nm_df['layer'] == li]\n",
    "            ax.plot(sub['step'], sub['norm_mean'], '-', label=f'L{li}')\n",
    "    ax.set_xlabel('Step'); ax.set_ylabel('Mean Norm')\n",
    "    ax.set_title('Activation Norms'); ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/physics_diagnostics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_comparison_table(wave_history, std_history):\n",
    "    \"\"\"Show a side-by-side pandas table of metrics.\"\"\"\n",
    "    rows = []\n",
    "    all_steps = sorted(set(\n",
    "        [h['step'] for h in wave_history] +\n",
    "        [h['step'] for h in std_history]\n",
    "    ))\n",
    "    wave_map = {h['step']: h for h in wave_history}\n",
    "    std_map = {h['step']: h for h in std_history}\n",
    "    for step in all_steps:\n",
    "        w = wave_map.get(step, {})\n",
    "        s = std_map.get(step, {})\n",
    "        rows.append({\n",
    "            'Step': step,\n",
    "            'Wave PPL': f\"{w['ppl']:.1f}\" if 'ppl' in w else '-',\n",
    "            'Std PPL': f\"{s['ppl']:.1f}\" if 'ppl' in s else '-',\n",
    "            'Wave Acc%': f\"{w['acc']:.1f}\" if 'acc' in w else '-',\n",
    "            'Std Acc%': f\"{s['acc']:.1f}\" if 'acc' in s else '-',\n",
    "            'Wave Loss': f\"{w['val_loss']:.3f}\" if 'val_loss' in w else '-',\n",
    "            'Std Loss': f\"{s['val_loss']:.3f}\" if 'val_loss' in s else '-',\n",
    "        })\n",
    "    display(pd.DataFrame(rows).style.set_caption(\n",
    "        'Wave Field vs Standard â€” Training Progress'))\n",
    "\n",
    "\n",
    "print(\"Visualization helpers ready\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7: CREATE MODELS + PARAM COUNT\n",
    "# ============================================================\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Wave Field V4.3.3 (SPECTRE)\n",
    "wave_model = WaveFieldTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=CFG['embed_dim'],\n",
    "    num_layers=CFG['num_layers'],\n",
    "    num_heads=CFG['num_heads'],\n",
    "    ffn_dim=CFG['ffn_dim'],\n",
    "    field_size=CFG['field_size'],\n",
    "    max_seq_len=CFG['seq_len'] + 2,\n",
    "    dropout=0.1,\n",
    "    use_checkpoint=True,\n",
    "    interference_interval=3,\n",
    "    n_components=1,\n",
    "    local_window=0,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Standard Transformer baseline\n",
    "std_model = StandardTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=CFG['embed_dim'],\n",
    "    num_layers=CFG['num_layers'],\n",
    "    num_heads=CFG['num_heads'],\n",
    "    ffn_dim=CFG['ffn_dim'],\n",
    "    max_seq_len=CFG['seq_len'] + 2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# torch.compile for Wave (non-FFT submodules only)\n",
    "try:\n",
    "    wave_model.compile_model(mode='reduce-overhead')\n",
    "    compile_status = 'enabled'\n",
    "except Exception as e:\n",
    "    compile_status = f'skipped ({e})'\n",
    "\n",
    "wave_params = sum(p.numel() for p in wave_model.parameters())\n",
    "std_params = sum(p.numel() for p in std_model.parameters())\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style='background:#1a1a2e;color:#e0e0e0;padding:15px;border-radius:8px;font-family:monospace'>\n",
    "  <h3 style='color:#00d4ff;margin:0 0 10px 0'>Model Summary â€” Scale {SCALE}</h3>\n",
    "  <table style='color:#e0e0e0;border-collapse:collapse;width:100%'>\n",
    "    <tr style='border-bottom:1px solid #444'>\n",
    "      <th style='text-align:left;padding:5px'>Model</th>\n",
    "      <th style='text-align:right;padding:5px'>Params</th>\n",
    "      <th style='text-align:right;padding:5px'>Complexity</th>\n",
    "      <th style='text-align:right;padding:5px'>torch.compile</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style='padding:5px;color:#ff6b6b'>Wave Field V4.3.3</td>\n",
    "      <td style='padding:5px;text-align:right'><b>{wave_params:,}</b> ({wave_params/1e6:.1f}M)</td>\n",
    "      <td style='padding:5px;text-align:right'>O(n log n)</td>\n",
    "      <td style='padding:5px;text-align:right'>{compile_status}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style='padding:5px;color:#4ecdc4'>Standard Transformer</td>\n",
    "      <td style='padding:5px;text-align:right'><b>{std_params:,}</b> ({std_params/1e6:.1f}M)</td>\n",
    "      <td style='padding:5px;text-align:right'>O(n^2)</td>\n",
    "      <td style='padding:5px;text-align:right'>N/A</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  <p style='margin:10px 0 0 0;color:#888'>Overhead: Wave has +{(wave_params-std_params)/1e6:.1f}M from feature maps, spectral gate, interference</p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "# VRAM sanity check\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "dummy = torch.randint(0, VOCAB_SIZE, (CFG['batch_size'], CFG['seq_len']), device=device)\n",
    "with torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "    _ = wave_model(dummy)\n",
    "peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "print(f\"Wave forward pass peak VRAM: {peak:.2f} GB / {vram_gb:.1f} GB\")\n",
    "del dummy, _\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: TRAIN WAVE FIELD V4.3.3\n",
    "# ============================================================\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# V4.3.2 per-group LR: kernel params at 50x, QKV at 3x\n",
    "wave_optimizer = wave_model.configure_optimizer(\n",
    "    base_lr=CFG['peak_lr'], kernel_lr_mult=50.0, qk_lr_mult=3.0)\n",
    "wave_scheduler = WarmupCosineScheduler(\n",
    "    wave_optimizer, warmup=max(CFG['total_steps'] // 10, 100),\n",
    "    total=CFG['total_steps'])\n",
    "wave_scaler = torch.amp.GradScaler('cuda', enabled=USE_FP16)\n",
    "wave_monitor = PhysicsMonitor()\n",
    "\n",
    "wave_history = []\n",
    "batch_size = CFG['batch_size']\n",
    "grad_accum = CFG['grad_accum']\n",
    "total_steps = CFG['total_steps']\n",
    "eval_every = CFG['eval_every']\n",
    "ckpt_every = CFG['ckpt_every']\n",
    "\n",
    "# Initial eval\n",
    "vl, vp, va = evaluate(wave_model, val_x, val_y, batch_size, VOCAB_SIZE)\n",
    "wave_history.append({'step': 0, 'val_loss': vl, 'ppl': vp, 'acc': va, 'tps': 0})\n",
    "wave_monitor.snapshot(wave_model, 0, sample_x=train_x[:2])\n",
    "print(f\"Wave init: PPL {vp:.1f} | Acc {va:.1f}%\")\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "accum_count = 0\n",
    "best_wave_ppl = float('inf')\n",
    "t0 = time.time()\n",
    "wave_model.train()\n",
    "wave_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "pbar = tqdm(total=total_steps, desc='Wave Training', unit='step')\n",
    "\n",
    "while step < total_steps:\n",
    "    for x, y in make_batches_gpu(train_x, train_y, batch_size):\n",
    "        if step >= total_steps:\n",
    "            break\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            logits, _ = wave_model(x)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, VOCAB_SIZE), y.reshape(-1))\n",
    "            loss_scaled = loss / grad_accum\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        wave_scaler.scale(loss_scaled).backward()\n",
    "        accum_count += 1\n",
    "\n",
    "        if accum_count % grad_accum == 0:\n",
    "            wave_scaler.unscale_(wave_optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(wave_model.parameters(), 1.0)\n",
    "            wave_scaler.step(wave_optimizer)\n",
    "            wave_scaler.update()\n",
    "            wave_scheduler.step()\n",
    "            wave_optimizer.zero_grad(set_to_none=True)\n",
    "            step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            elapsed = time.time() - t0\n",
    "            tok_seen = step * batch_size * grad_accum * CFG['seq_len']\n",
    "            tps = tok_seen / elapsed if elapsed > 0 else 0\n",
    "            lr_now = wave_scheduler.get_lr()[0]\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.3f}',\n",
    "                'lr': f'{lr_now:.1e}',\n",
    "                'tok/s': f'{tps/1e3:.0f}K',\n",
    "                'VRAM': f'{torch.cuda.memory_allocated()/1e9:.1f}G'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Eval + diagnostics checkpoint\n",
    "            if step % eval_every == 0 or step == total_steps:\n",
    "                wave_model.eval()\n",
    "                vl, vp, va = evaluate(wave_model, val_x, val_y, batch_size, VOCAB_SIZE)\n",
    "                wave_monitor.snapshot(wave_model, step, sample_x=train_x[:2])\n",
    "                wave_history.append({\n",
    "                    'step': step, 'val_loss': vl, 'ppl': vp, 'acc': va,\n",
    "                    'tps': tps, 'tok_M': tok_seen / 1e6, 'time_s': elapsed\n",
    "                })\n",
    "                mark = ''\n",
    "                if vp < best_wave_ppl:\n",
    "                    best_wave_ppl = vp\n",
    "                    torch.save(wave_model.state_dict(),\n",
    "                               CKPT_DIR / f'wave_{SCALE}_best.pt')\n",
    "                    mark = ' *BEST'\n",
    "                tqdm.write(f\"  [Wave] Step {step}/{total_steps} | \"\n",
    "                           f\"PPL {vp:.1f} | Acc {va:.1f}% | \"\n",
    "                           f\"{tok_seen/1e6:.1f}M tok | \"\n",
    "                           f\"{tps/1e3:.0f}K tok/s{mark}\")\n",
    "\n",
    "                # Generation sample\n",
    "                sample = generate_text(wave_model, raw_tok,\n",
    "                                       prompt='The world', max_tokens=50)\n",
    "                tqdm.write(f\"  [Gen] {sample[:150]}\")\n",
    "                wave_model.train()\n",
    "\n",
    "            # Save checkpoint\n",
    "            if step % ckpt_every == 0:\n",
    "                torch.save({\n",
    "                    'model': wave_model.state_dict(),\n",
    "                    'optimizer': wave_optimizer.state_dict(),\n",
    "                    'scaler': wave_scaler.state_dict(),\n",
    "                    'step': step, 'history': wave_history\n",
    "                }, CKPT_DIR / f'wave_{SCALE}_step{step}.pt')\n",
    "\n",
    "pbar.close()\n",
    "wave_peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "wave_total_time = time.time() - t0\n",
    "print(f\"\\nWave training complete: {wave_total_time:.0f}s | Peak VRAM: {wave_peak_mem:.2f} GB\")\n",
    "print(f\"Best PPL: {best_wave_ppl:.1f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9: WAVE PHYSICS DIAGNOSTICS\n",
    "# ============================================================\n",
    "plot_physics_diagnostics(wave_monitor, title=f'Wave Field V4.3.3 Physics â€” {SCALE}')\n",
    "\n",
    "# Kernel parameter table at final step\n",
    "df = wave_monitor.summary_df()\n",
    "final = df[df['step'] == df['step'].max()]\n",
    "display(final[['layer', 'head', 'freq', 'damp', 'phase',\n",
    "               'output_rank', 'norm_mean']].style.set_caption(\n",
    "    'Final Kernel Parameters').format({\n",
    "        'freq': '{:.3f}', 'damp': '{:.3f}', 'phase': '{:.3f}',\n",
    "        'output_rank': '{:.0f}', 'norm_mean': '{:.1f}'\n",
    "    }).background_gradient(subset=['damp'], cmap='RdYlGn_r')\n",
    "     .background_gradient(subset=['output_rank'], cmap='RdYlGn'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 10: TRAIN STANDARD TRANSFORMER\n",
    "# ============================================================\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "std_optimizer = torch.optim.AdamW(std_model.parameters(),\n",
    "                                   lr=CFG['peak_lr'], weight_decay=0.01)\n",
    "std_scheduler = WarmupCosineScheduler(\n",
    "    std_optimizer, warmup=max(CFG['total_steps'] // 10, 100),\n",
    "    total=CFG['total_steps'])\n",
    "std_scaler = torch.amp.GradScaler('cuda', enabled=USE_FP16)\n",
    "\n",
    "std_history = []\n",
    "\n",
    "# Initial eval\n",
    "vl, vp, va = evaluate(std_model, val_x, val_y, batch_size, VOCAB_SIZE)\n",
    "std_history.append({'step': 0, 'val_loss': vl, 'ppl': vp, 'acc': va, 'tps': 0})\n",
    "print(f\"Std init: PPL {vp:.1f} | Acc {va:.1f}%\")\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "accum_count = 0\n",
    "best_std_ppl = float('inf')\n",
    "t0 = time.time()\n",
    "std_model.train()\n",
    "std_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "pbar = tqdm(total=total_steps, desc='Standard Training', unit='step')\n",
    "\n",
    "while step < total_steps:\n",
    "    for x, y in make_batches_gpu(train_x, train_y, batch_size):\n",
    "        if step >= total_steps:\n",
    "            break\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            logits, _ = std_model(x)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, VOCAB_SIZE), y.reshape(-1))\n",
    "            loss_scaled = loss / grad_accum\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        std_scaler.scale(loss_scaled).backward()\n",
    "        accum_count += 1\n",
    "\n",
    "        if accum_count % grad_accum == 0:\n",
    "            std_scaler.unscale_(std_optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(std_model.parameters(), 1.0)\n",
    "            std_scaler.step(std_optimizer)\n",
    "            std_scaler.update()\n",
    "            std_scheduler.step()\n",
    "            std_optimizer.zero_grad(set_to_none=True)\n",
    "            step += 1\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            tok_seen = step * batch_size * grad_accum * CFG['seq_len']\n",
    "            tps = tok_seen / elapsed if elapsed > 0 else 0\n",
    "            lr_now = std_scheduler.get_lr()[0]\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.3f}',\n",
    "                'lr': f'{lr_now:.1e}',\n",
    "                'tok/s': f'{tps/1e3:.0f}K',\n",
    "                'VRAM': f'{torch.cuda.memory_allocated()/1e9:.1f}G'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            if step % eval_every == 0 or step == total_steps:\n",
    "                std_model.eval()\n",
    "                vl, vp, va = evaluate(std_model, val_x, val_y, batch_size, VOCAB_SIZE)\n",
    "                std_history.append({\n",
    "                    'step': step, 'val_loss': vl, 'ppl': vp, 'acc': va,\n",
    "                    'tps': tps, 'tok_M': tok_seen / 1e6, 'time_s': elapsed\n",
    "                })\n",
    "                mark = ''\n",
    "                if vp < best_std_ppl:\n",
    "                    best_std_ppl = vp\n",
    "                    torch.save(std_model.state_dict(),\n",
    "                               CKPT_DIR / f'std_{SCALE}_best.pt')\n",
    "                    mark = ' *BEST'\n",
    "                tqdm.write(f\"  [Std] Step {step}/{total_steps} | \"\n",
    "                           f\"PPL {vp:.1f} | Acc {va:.1f}% | \"\n",
    "                           f\"{tok_seen/1e6:.1f}M tok | \"\n",
    "                           f\"{tps/1e3:.0f}K tok/s{mark}\")\n",
    "\n",
    "                sample = generate_text(std_model, raw_tok,\n",
    "                                       prompt='The world', max_tokens=50)\n",
    "                tqdm.write(f\"  [Gen] {sample[:150]}\")\n",
    "                std_model.train()\n",
    "\n",
    "            if step % ckpt_every == 0:\n",
    "                torch.save({\n",
    "                    'model': std_model.state_dict(),\n",
    "                    'optimizer': std_optimizer.state_dict(),\n",
    "                    'scaler': std_scaler.state_dict(),\n",
    "                    'step': step, 'history': std_history\n",
    "                }, CKPT_DIR / f'std_{SCALE}_step{step}.pt')\n",
    "\n",
    "pbar.close()\n",
    "std_peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "std_total_time = time.time() - t0\n",
    "print(f\"\\nStandard training complete: {std_total_time:.0f}s | Peak VRAM: {std_peak_mem:.2f} GB\")\n",
    "print(f\"Best PPL: {best_std_ppl:.1f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 11: TRAINING COMPARISON â€” CURVES + TABLE\n",
    "# ============================================================\n",
    "plot_training_curves(wave_history, std_history, title_suffix=f'({SCALE})')\n",
    "display_comparison_table(wave_history, std_history)\n",
    "\n",
    "# Summary card\n",
    "display(HTML(f\"\"\"\n",
    "<div style='background:#1a1a2e;color:#e0e0e0;padding:15px;border-radius:8px;font-family:monospace;margin-top:10px'>\n",
    "  <h3 style='color:#00d4ff;margin:0 0 10px 0'>Training Summary â€” {SCALE}</h3>\n",
    "  <table style='color:#e0e0e0;border-collapse:collapse;width:100%'>\n",
    "    <tr style='border-bottom:1px solid #444'>\n",
    "      <th style='text-align:left;padding:5px'>Metric</th>\n",
    "      <th style='text-align:right;padding:5px;color:#ff6b6b'>Wave V4.3.3</th>\n",
    "      <th style='text-align:right;padding:5px;color:#4ecdc4'>Standard</th>\n",
    "    </tr>\n",
    "    <tr><td style='padding:5px'>Best PPL</td>\n",
    "        <td style='padding:5px;text-align:right'><b>{best_wave_ppl:.1f}</b></td>\n",
    "        <td style='padding:5px;text-align:right'><b>{best_std_ppl:.1f}</b></td></tr>\n",
    "    <tr><td style='padding:5px'>Parameters</td>\n",
    "        <td style='padding:5px;text-align:right'>{wave_params/1e6:.1f}M</td>\n",
    "        <td style='padding:5px;text-align:right'>{std_params/1e6:.1f}M</td></tr>\n",
    "    <tr><td style='padding:5px'>Peak VRAM</td>\n",
    "        <td style='padding:5px;text-align:right'>{wave_peak_mem:.2f} GB</td>\n",
    "        <td style='padding:5px;text-align:right'>{std_peak_mem:.2f} GB</td></tr>\n",
    "    <tr><td style='padding:5px'>Training Time</td>\n",
    "        <td style='padding:5px;text-align:right'>{wave_total_time:.0f}s</td>\n",
    "        <td style='padding:5px;text-align:right'>{std_total_time:.0f}s</td></tr>\n",
    "  </table>\n",
    "</div>\n",
    "\"\"\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 12: GENERATION COMPARISON\n",
    "# ============================================================\n",
    "prompts = [\n",
    "    \"The president of the United States\",\n",
    "    \"In the beginning, there was\",\n",
    "    \"Scientists discovered that\",\n",
    "    \"The most important thing about\",\n",
    "]\n",
    "\n",
    "gen_rows = []\n",
    "for prompt in prompts:\n",
    "    wave_text = generate_text(wave_model, raw_tok, prompt=prompt,\n",
    "                              max_tokens=60, temperature=0.8)\n",
    "    std_text = generate_text(std_model, raw_tok, prompt=prompt,\n",
    "                             max_tokens=60, temperature=0.8)\n",
    "    gen_rows.append({\n",
    "        'Prompt': prompt,\n",
    "        'Wave V4.3.3': wave_text[:200],\n",
    "        'Standard': std_text[:200]\n",
    "    })\n",
    "\n",
    "gen_df = pd.DataFrame(gen_rows)\n",
    "display(gen_df.style.set_caption('Generation Comparison (temp=0.8, top_k=40)')\n",
    "        .set_properties(**{'white-space': 'pre-wrap', 'max-width': '400px'}))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 13: SPEED & MEMORY SCALING BENCHMARK\n",
    "# ============================================================\n",
    "# Clean up training models to free VRAM for benchmarking\n",
    "del wave_optimizer, wave_scaler, std_optimizer, std_scaler\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "SPEED_SEQ_LENS = [256, 512, 1024, 2048, 4096, 8192]\n",
    "SPEED_BATCH = 2\n",
    "\n",
    "speed_results = []\n",
    "\n",
    "for sl in tqdm(SPEED_SEQ_LENS, desc='Speed benchmark'):\n",
    "    row = {'seq_len': sl}\n",
    "\n",
    "    # Standard\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    try:\n",
    "        m = StandardTransformer(\n",
    "            vocab_size=VOCAB_SIZE, embedding_dim=CFG['embed_dim'],\n",
    "            num_layers=CFG['num_layers'], num_heads=CFG['num_heads'],\n",
    "            ffn_dim=CFG['ffn_dim'], max_seq_len=sl+2, dropout=0.0\n",
    "        ).to(device).eval()\n",
    "        x = torch.randint(0, VOCAB_SIZE, (SPEED_BATCH, sl), device=device)\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            for _ in range(3): m(x)\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            for _ in range(10): m(x)\n",
    "        torch.cuda.synchronize()\n",
    "        row['std_ms'] = (time.time() - t0) / 10 * 1000\n",
    "        row['std_mem'] = torch.cuda.max_memory_allocated() / 1e9\n",
    "        del m, x\n",
    "    except RuntimeError:\n",
    "        row['std_ms'] = float('inf')\n",
    "        row['std_mem'] = float('inf')\n",
    "\n",
    "    # Wave\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    try:\n",
    "        fs = max(sl * 2, CFG['field_size'])\n",
    "        m = WaveFieldTransformer(\n",
    "            vocab_size=VOCAB_SIZE, embedding_dim=CFG['embed_dim'],\n",
    "            num_layers=CFG['num_layers'], num_heads=CFG['num_heads'],\n",
    "            ffn_dim=CFG['ffn_dim'], field_size=fs,\n",
    "            max_seq_len=sl+2, dropout=0.0, use_checkpoint=False,\n",
    "            interference_interval=3, n_components=1, local_window=0,\n",
    "            device=device\n",
    "        ).to(device).eval()\n",
    "        x = torch.randint(0, VOCAB_SIZE, (SPEED_BATCH, sl), device=device)\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            for _ in range(3): m(x)\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda', enabled=USE_FP16):\n",
    "            for _ in range(10): m(x)\n",
    "        torch.cuda.synchronize()\n",
    "        row['wave_ms'] = (time.time() - t0) / 10 * 1000\n",
    "        row['wave_mem'] = torch.cuda.max_memory_allocated() / 1e9\n",
    "        del m, x\n",
    "    except RuntimeError:\n",
    "        row['wave_ms'] = float('inf')\n",
    "        row['wave_mem'] = float('inf')\n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    if row['std_ms'] < float('inf') and row['wave_ms'] < float('inf'):\n",
    "        row['speedup'] = row['std_ms'] / row['wave_ms']\n",
    "    else:\n",
    "        row['speedup'] = None\n",
    "    speed_results.append(row)\n",
    "\n",
    "speed_df = pd.DataFrame(speed_results)\n",
    "\n",
    "# Display table\n",
    "def fmt_ms(v): return 'OOM' if v == float('inf') else f'{v:.1f}'\n",
    "def fmt_gb(v): return 'OOM' if v == float('inf') else f'{v:.2f}'\n",
    "\n",
    "disp = speed_df.copy()\n",
    "disp['std_ms'] = disp['std_ms'].apply(fmt_ms)\n",
    "disp['wave_ms'] = disp['wave_ms'].apply(fmt_ms)\n",
    "disp['std_mem'] = disp['std_mem'].apply(fmt_gb)\n",
    "disp['wave_mem'] = disp['wave_mem'].apply(fmt_gb)\n",
    "disp['speedup'] = disp['speedup'].apply(lambda v: f'{v:.2f}x' if v else '-')\n",
    "disp.columns = ['Seq Len', 'Std (ms)', 'Std VRAM', 'Wave (ms)', 'Wave VRAM', 'Speedup']\n",
    "display(disp.style.set_caption(f'Speed & Memory Benchmark (B={SPEED_BATCH}, fp16)'))\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "valid = speed_df[(speed_df['std_ms'] < float('inf')) & (speed_df['wave_ms'] < float('inf'))]\n",
    "if not valid.empty:\n",
    "    ax1.plot(valid['seq_len'], valid['std_ms'], 'b-o', label='Standard O(n^2)', linewidth=2)\n",
    "    ax1.plot(valid['seq_len'], valid['wave_ms'], 'r-o', label='Wave O(n log n)', linewidth=2)\n",
    "ax1.set_xlabel('Sequence Length'); ax1.set_ylabel('Forward Pass (ms)')\n",
    "ax1.set_title('Speed Scaling'); ax1.legend()\n",
    "ax1.grid(True, alpha=0.3); ax1.set_yscale('log'); ax1.set_xscale('log', base=2)\n",
    "\n",
    "valid_mem = speed_df[(speed_df['std_mem'] < float('inf')) & (speed_df['wave_mem'] < float('inf'))]\n",
    "if not valid_mem.empty:\n",
    "    ax2.plot(valid_mem['seq_len'], valid_mem['std_mem'], 'b-o', label='Standard O(n^2)', linewidth=2)\n",
    "    ax2.plot(valid_mem['seq_len'], valid_mem['wave_mem'], 'r-o', label='Wave O(n log n)', linewidth=2)\n",
    "ax2.axhline(y=vram_gb, color='gray', linestyle='--', alpha=0.7, label=f'{gpu_name} ({vram_gb:.0f} GB)')\n",
    "ax2.set_xlabel('Sequence Length'); ax2.set_ylabel('Peak VRAM (GB)')\n",
    "ax2.set_title('Memory Scaling'); ax2.legend()\n",
    "ax2.grid(True, alpha=0.3); ax2.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/speed_memory_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 14: KERNEL HEALTH â€” FROZEN OR EVOLVING?\n",
    "# ============================================================\n",
    "df = wave_monitor.summary_df()\n",
    "if len(df['step'].unique()) >= 2:\n",
    "    init_df = df[df['step'] == df['step'].min()]\n",
    "    final_df = df[df['step'] == df['step'].max()]\n",
    "\n",
    "    merged = init_df.merge(final_df, on=['layer', 'head'],\n",
    "                           suffixes=('_init', '_final'))\n",
    "\n",
    "    merged['freq_change'] = (merged['freq_final'] - merged['freq_init']).abs()\n",
    "    merged['damp_change'] = (merged['damp_final'] - merged['damp_init']).abs()\n",
    "\n",
    "    avg_freq_change = merged['freq_change'].mean()\n",
    "    avg_damp_change = merged['damp_change'].mean()\n",
    "\n",
    "    frozen_threshold = 0.01\n",
    "    n_frozen_freq = (merged['freq_change'] < frozen_threshold).sum()\n",
    "    total_heads = len(merged)\n",
    "\n",
    "    health = 'HEALTHY' if n_frozen_freq < total_heads * 0.3 else 'FROZEN'\n",
    "    color = '#00ff00' if health == 'HEALTHY' else '#ff4444'\n",
    "\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style='background:#1a1a2e;color:#e0e0e0;padding:15px;border-radius:8px;font-family:monospace'>\n",
    "      <h3 style='color:{color};margin:0'>Kernel Health: {health}</h3>\n",
    "      <p>Avg frequency change: <b>{avg_freq_change:.4f}</b> | \n",
    "         Avg damping change: <b>{avg_damp_change:.4f}</b></p>\n",
    "      <p>Frozen heads: {n_frozen_freq}/{total_heads} (threshold: change &lt; {frozen_threshold})</p>\n",
    "      <p style='color:#888;margin-top:5px'>V4.3.2 fix: kernel_lr = 50x base_lr prevents freezing.</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    # Heatmaps\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "    freq_pivot = merged.pivot(index='layer', columns='head', values='freq_change')\n",
    "    damp_pivot = merged.pivot(index='layer', columns='head', values='damp_change')\n",
    "\n",
    "    im1 = ax1.imshow(freq_pivot.values, cmap='YlOrRd', aspect='auto')\n",
    "    ax1.set_xlabel('Head'); ax1.set_ylabel('Layer')\n",
    "    ax1.set_title('Frequency Change (init -> final)')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    im2 = ax2.imshow(damp_pivot.values, cmap='YlOrRd', aspect='auto')\n",
    "    ax2.set_xlabel('Head'); ax2.set_ylabel('Layer')\n",
    "    ax2.set_title('Damping Change (init -> final)')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/kernel_health.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 snapshots for kernel health analysis.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 15: SAVE ALL RESULTS + FINAL VERDICT\n",
    "# ============================================================\n",
    "\n",
    "all_results = {\n",
    "    'config': {\n",
    "        'scale': SCALE,\n",
    "        'embed_dim': CFG['embed_dim'],\n",
    "        'num_layers': CFG['num_layers'],\n",
    "        'num_heads': CFG['num_heads'],\n",
    "        'ffn_dim': CFG['ffn_dim'],\n",
    "        'field_size': CFG['field_size'],\n",
    "        'seq_len': CFG['seq_len'],\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'total_steps': CFG['total_steps'],\n",
    "        'wave_params': wave_params,\n",
    "        'std_params': std_params,\n",
    "        'gpu': gpu_name,\n",
    "        'vram_gb': vram_gb,\n",
    "    },\n",
    "    'wave': {\n",
    "        'best_ppl': best_wave_ppl,\n",
    "        'peak_mem_gb': wave_peak_mem,\n",
    "        'time_s': wave_total_time,\n",
    "        'history': wave_history,\n",
    "    },\n",
    "    'standard': {\n",
    "        'best_ppl': best_std_ppl,\n",
    "        'peak_mem_gb': std_peak_mem,\n",
    "        'time_s': std_total_time,\n",
    "        'history': std_history,\n",
    "    },\n",
    "    'speed_benchmark': speed_results,\n",
    "    'physics_snapshots': wave_monitor.snapshots,\n",
    "}\n",
    "\n",
    "with open(f'results/colab_{SCALE}_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved: results/colab_{SCALE}_results.json\")\n",
    "print(f\"Checkpoints in: {CKPT_DIR}/\")\n",
    "print(f\"Plots in: results/\")\n",
    "\n",
    "# Final verdict\n",
    "if best_wave_ppl < best_std_ppl:\n",
    "    verdict = 'Wave WINS'\n",
    "    color = '#00ff00'\n",
    "elif best_wave_ppl < best_std_ppl * 1.5:\n",
    "    verdict = 'COMPETITIVE'\n",
    "    color = '#ffaa00'\n",
    "else:\n",
    "    verdict = 'Standard leads (need more data/scale)'\n",
    "    color = '#ff6666'\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style='background:#0a0a1a;color:#e0e0e0;padding:20px;border-radius:12px;\n",
    "            font-family:monospace;border:2px solid {color};margin-top:10px'>\n",
    "  <h2 style='color:{color};margin:0 0 10px 0;text-align:center'>{verdict}</h2>\n",
    "  <table style='color:#e0e0e0;width:100%;border-collapse:collapse'>\n",
    "    <tr><td style='padding:5px'>Wave V4.3.3 Best PPL</td>\n",
    "        <td style='padding:5px;text-align:right;font-size:1.2em'><b>{best_wave_ppl:.1f}</b></td></tr>\n",
    "    <tr><td style='padding:5px'>Standard Best PPL</td>\n",
    "        <td style='padding:5px;text-align:right;font-size:1.2em'><b>{best_std_ppl:.1f}</b></td></tr>\n",
    "    <tr style='border-top:1px solid #444'>\n",
    "        <td style='padding:5px'>Wave Params</td>\n",
    "        <td style='padding:5px;text-align:right'>{wave_params/1e6:.1f}M</td></tr>\n",
    "    <tr><td style='padding:5px'>Standard Params</td>\n",
    "        <td style='padding:5px;text-align:right'>{std_params/1e6:.1f}M</td></tr>\n",
    "    <tr><td style='padding:5px'>Wave Complexity</td>\n",
    "        <td style='padding:5px;text-align:right'>O(n log n)</td></tr>\n",
    "    <tr><td style='padding:5px'>Standard Complexity</td>\n",
    "        <td style='padding:5px;text-align:right'>O(n^2)</td></tr>\n",
    "  </table>\n",
    "  <p style='color:#888;text-align:center;margin:10px 0 0 0'>\n",
    "    Scale {SCALE} | {CFG['embed_dim']}d {CFG['num_layers']}L {CFG['num_heads']}H | \n",
    "    {gpu_name} | WikiText-103\n",
    "  </p>\n",
    "</div>\n",
    "\"\"\"))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}