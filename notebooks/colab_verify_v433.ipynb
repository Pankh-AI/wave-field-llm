{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave Field LLM -- V4.3.3 S1 Benchmark\n",
    "\n",
    "Reproduces the S1 result: **SPECTRE-Wave PPL 239 vs Standard PPL 171** (1.40x gap)\n",
    "\n",
    "Runtime: ~25 min on T4, ~12 min on A100"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "!git clone https://github.com/Pankh-AI/wave-field-llm.git\n",
    "%cd wave-field-llm\n",
    "!pip install -q tokenizers datasets\n",
    "\n",
    "# Verify we have V4.3.3 code\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "from src import __version__\n",
    "print(f'\\nCode version: {__version__}')\n",
    "assert __version__ == '4.3.3', f'ERROR: Expected V4.3.3, got {__version__}!'\n",
    "print('Version check PASSED')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Check GPU\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU: {gpu_name}')\n",
    "    print(f'VRAM: {vram_gb:.1f} GB')\n",
    "else:\n",
    "    raise RuntimeError('No GPU! Go to Runtime > Change runtime type > T4 GPU')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 3: Run S1 benchmark (~25 min on T4)\nimport os\nos.environ['SCALE'] = 'S1'              # S1 only (22M params, 20M tokens)\nos.environ['DATASET'] = '2'             # WikiText-2 (matches verified PPL 229 result)\nos.environ['MONITOR'] = '0'             # Skip monitor for speed\nos.environ['BATCH_SIZE'] = '16'\n# os.environ['MODEL'] = 'wave'          # Uncomment to run Wave only (skip Standard)\n\n!python benchmarks/benchmark_scaling.py",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4: Results\n",
    "import json\n",
    "\n",
    "with open('results/scaling_s1.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('=' * 60)\n",
    "print('  S1 RESULTS (22M params, 20M tokens)')\n",
    "print('=' * 60)\n",
    "for r in data['results']:\n",
    "    print(f\"\\n  {r['run_name']}\")\n",
    "    print(f\"    PPL:    {r['best_ppl']:.2f}\")\n",
    "    print(f\"    Acc:    {r['best_acc']:.2f}%\")\n",
    "    print(f\"    Params: {r['params']:,}\")\n",
    "    print(f\"    Speed:  {r['tokens_per_sec']:,} tok/s\")\n",
    "    print(f\"    Time:   {r['total_time_s']:.0f}s\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}