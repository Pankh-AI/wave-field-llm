{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wave Field LLM — S3 Benchmark (100M params, 100M tokens)\n",
        "\n",
        "V4.3.7 SPECTRE-Wave on Colab T4 (16GB). Optimized for free tier.\n",
        "\n",
        "| | S1 | S2 | **S3 (this)** |\n",
        "|---|---|---|---|\n",
        "| Params | 22M | 55M | **100M** |\n",
        "| Tokens | 20M | 50M | **100M** |\n",
        "| T4 time | 25 min | 2.5 hrs | **~2.5 hrs** |\n",
        "\n",
        "**Optimizations:** batch=20 (T4 has 16GB vs 6GB 3060), local checkpoints + Drive backup every 15 min, monitor off.\n",
        "\n",
        "**If disconnected:** Re-run all cells. Auto-resumes from Drive checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Setup everything (Drive + repo + deps + GPU check)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, shutil, subprocess\n",
        "\n",
        "# Drive dirs for persistence\n",
        "DRIVE_DIR = '/content/drive/MyDrive/wave-field-llm'\n",
        "DRIVE_CKPT = os.path.join(DRIVE_DIR, 'checkpoints')\n",
        "DRIVE_CACHE = os.path.join(DRIVE_DIR, 'cache')\n",
        "os.makedirs(DRIVE_CKPT, exist_ok=True)\n",
        "os.makedirs(DRIVE_CACHE, exist_ok=True)\n",
        "\n",
        "# Clone / update repo\n",
        "if not os.path.isdir('/content/wave-field-llm'):\n",
        "    !git clone https://github.com/Pankh-AI/wave-field-llm.git /content/wave-field-llm\n",
        "else:\n",
        "    !cd /content/wave-field-llm && git pull --ff-only\n",
        "%cd /content/wave-field-llm\n",
        "!pip install -q tokenizers datasets\n",
        "\n",
        "# Version check\n",
        "import sys; sys.path.insert(0, '.')\n",
        "from src import __version__\n",
        "print(f'\\nV{__version__}')\n",
        "assert __version__ >= '4.3.7', f'Need V4.3.7+, got {__version__}'\n",
        "\n",
        "# GPU\n",
        "import torch\n",
        "assert torch.cuda.is_available(), 'No GPU! Runtime > Change runtime type > T4'\n",
        "gpu = torch.cuda.get_device_name()\n",
        "vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f'{gpu} — {vram:.1f} GB VRAM')\n",
        "\n",
        "# Optimal batch size for available VRAM\n",
        "# S3: 100M params. With gradient checkpointing:\n",
        "#   batch=20 uses ~7-8GB, batch=24 uses ~9-10GB, batch=28 uses ~11-12GB\n",
        "if vram >= 36:      # A100 40/80GB\n",
        "    BATCH = 48\n",
        "elif vram >= 14:    # T4 16GB\n",
        "    BATCH = 20\n",
        "elif vram >= 10:    # various 12GB GPUs\n",
        "    BATCH = 12\n",
        "else:\n",
        "    BATCH = 8\n",
        "print(f'Batch size: {BATCH} (auto-selected for {vram:.0f}GB)')\n",
        "print(f'Tokens/step: {BATCH * 512:,} — steps: {100_000_000 // (BATCH * 512):,}')\n",
        "print(f'Estimated time: ~{100_000_000 / (BATCH * 512) * 0.4 / 60:.0f}-{100_000_000 / (BATCH * 512) * 0.6 / 60:.0f} min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Prepare local dirs + restore Drive cache\n",
        "import os, shutil\n",
        "\n",
        "# Local dirs (fast SSD)\n",
        "os.makedirs('results/checkpoints', exist_ok=True)\n",
        "os.makedirs('results/cache', exist_ok=True)\n",
        "os.makedirs('results/data', exist_ok=True)\n",
        "os.makedirs('results/monitor', exist_ok=True)\n",
        "\n",
        "# Restore tokenizer cache from Drive (avoids re-tokenizing 133M tokens)\n",
        "for f in os.listdir(DRIVE_CACHE):\n",
        "    src = os.path.join(DRIVE_CACHE, f)\n",
        "    dst = os.path.join('results/cache', f)\n",
        "    if not os.path.exists(dst):\n",
        "        print(f'Restoring cache: {f}')\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "# Restore checkpoint from Drive for resume\n",
        "resumed = False\n",
        "for f in ['spectre-wave_s3_resume.pt', 'spectre-wave_s3.pt']:\n",
        "    src = os.path.join(DRIVE_CKPT, f)\n",
        "    dst = os.path.join('results/checkpoints', f)\n",
        "    if os.path.exists(src) and not os.path.exists(dst):\n",
        "        print(f'Restoring checkpoint: {f} ({os.path.getsize(src)/1e6:.0f} MB)')\n",
        "        shutil.copy2(src, dst)\n",
        "        resumed = True\n",
        "\n",
        "if resumed:\n",
        "    import torch\n",
        "    ckpt = torch.load('results/checkpoints/spectre-wave_s3_resume.pt', map_location='cpu', weights_only=False)\n",
        "    print(f'\\nResuming from step {ckpt[\"step\"]}, {ckpt[\"tokens_seen\"]/1e6:.1f}M tokens, best PPL {ckpt[\"best_ppl\"]:.2f}')\n",
        "    del ckpt\n",
        "else:\n",
        "    print('Fresh start (no checkpoint on Drive)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Train S3 with Drive backup thread\n",
        "import os, shutil, threading, time\n",
        "\n",
        "# Background thread: copy checkpoints to Drive every 15 min\n",
        "# Training saves locally (fast), we backup to Drive periodically (slow but non-blocking)\n",
        "_stop_backup = threading.Event()\n",
        "\n",
        "def _drive_backup_loop():\n",
        "    while not _stop_backup.is_set():\n",
        "        _stop_backup.wait(900)  # 15 min\n",
        "        if _stop_backup.is_set():\n",
        "            break\n",
        "        for f in os.listdir('results/checkpoints'):\n",
        "            if f.startswith('spectre-wave_s3'):\n",
        "                src = os.path.join('results/checkpoints', f)\n",
        "                dst = os.path.join(DRIVE_CKPT, f)\n",
        "                try:\n",
        "                    shutil.copy2(src, dst)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        # Also backup tokenizer cache (one-time, small)\n",
        "        for f in os.listdir('results/cache'):\n",
        "            src = os.path.join('results/cache', f)\n",
        "            dst = os.path.join(DRIVE_CACHE, f)\n",
        "            if not os.path.exists(dst):\n",
        "                try:\n",
        "                    shutil.copy2(src, dst)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "backup_thread = threading.Thread(target=_drive_backup_loop, daemon=True)\n",
        "backup_thread.start()\n",
        "print('Drive backup thread started (every 15 min)')\n",
        "\n",
        "# Run training\n",
        "os.environ['SCALE'] = 'S3'\n",
        "os.environ['MODEL'] = 'wave'\n",
        "os.environ['DATASET'] = '103'\n",
        "os.environ['RESUME'] = '1'\n",
        "os.environ['MONITOR'] = '0'            # OFF for speed (saves I/O + ~5% GPU)\n",
        "os.environ['BATCH_SIZE'] = str(BATCH)   # T4-optimized batch size\n",
        "\n",
        "!python benchmarks/benchmark_scaling.py\n",
        "\n",
        "# Stop backup thread\n",
        "_stop_backup.set()\n",
        "\n",
        "# Final Drive save (training complete)\n",
        "print('\\nSaving final checkpoints to Drive...')\n",
        "for f in os.listdir('results/checkpoints'):\n",
        "    if 's3' in f:\n",
        "        shutil.copy2(os.path.join('results/checkpoints', f), os.path.join(DRIVE_CKPT, f))\n",
        "        print(f'  Saved: {f}')\n",
        "for f in os.listdir('results/cache'):\n",
        "    dst = os.path.join(DRIVE_CACHE, f)\n",
        "    if not os.path.exists(dst):\n",
        "        shutil.copy2(os.path.join('results/cache', f), dst)\n",
        "# Copy results JSON\n",
        "for f in ['scaling_s3.json', 'scaling_benchmark.json']:\n",
        "    src = os.path.join('results/data', f)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, os.path.join(DRIVE_DIR, f))\n",
        "        print(f'  Saved: {f}')\n",
        "print('Done! Results on Drive.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Results\n",
        "import json, os\n",
        "\n",
        "for path in ['results/data/scaling_s3.json', 'results/data/scaling_benchmark.json']:\n",
        "    if not os.path.exists(path):\n",
        "        continue\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    print('=' * 60)\n",
        "    print(f'  S3 RESULTS — {data[\"metadata\"].get(\"dataset\", \"?\")} — {data[\"metadata\"].get(\"gpu\", \"?\")}')\n",
        "    print('=' * 60)\n",
        "    for r in data['results']:\n",
        "        print(f'\\n  {r[\"run_name\"]}')\n",
        "        print(f'    PPL: {r[\"best_ppl\"]:.2f}  Acc: {r[\"best_acc\"]:.1f}%  Params: {r[\"params\"]:,}')\n",
        "        print(f'    Speed: {r[\"tokens_per_sec\"]:,} tok/s  Time: {r[\"total_time_s\"]/60:.0f} min')\n",
        "        curve = r.get('curve', [])\n",
        "        if curve:\n",
        "            print(f'\\n    {\"Tokens\":>8} {\"PPL\":>8} {\"Acc\":>7}')\n",
        "            for pt in curve:\n",
        "                print(f'    {pt[\"tokens_M\"]:>6.1f}M {pt[\"ppl\"]:>8.2f} {pt[\"acc\"]:>6.1f}%')\n",
        "    break\n",
        "else:\n",
        "    # No results file — check checkpoint\n",
        "    ckpt_path = 'results/checkpoints/spectre-wave_s3_resume.pt'\n",
        "    if os.path.exists(ckpt_path):\n",
        "        import torch\n",
        "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
        "        total_steps = 100_000_000 // (BATCH * 512)\n",
        "        pct = ckpt['tokens_seen'] / 100e6 * 100\n",
        "        print(f'Training incomplete ({pct:.0f}% done)')\n",
        "        print(f'  Step {ckpt[\"step\"]}/{total_steps} | {ckpt[\"tokens_seen\"]/1e6:.1f}M/100M tokens')\n",
        "        print(f'  Best PPL: {ckpt[\"best_ppl\"]:.2f} | Best Acc: {ckpt[\"best_acc\"]:.1f}%')\n",
        "        print(f'  Re-run Cell 3 to continue.')\n",
        "    else:\n",
        "        print('No results or checkpoint found. Run Cell 3 first.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Verify results (run after training completes)\n",
        "!python tests/test_causality.py\n",
        "print('\\n' + '='*60)\n",
        "!python tests/verify_results.py --scale S3 --dataset 103 --skip-generation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Download results\n",
        "import shutil, os\n",
        "from google.colab import files\n",
        "\n",
        "# Save everything to Drive first\n",
        "for f in ['scaling_s3.json', 'scaling_benchmark.json', 'verification_s3.json']:\n",
        "    src = os.path.join('results/data', f)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, os.path.join(DRIVE_DIR, f))\n",
        "        files.download(src)\n",
        "        print(f'Saved + downloaded: {f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
