{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Wave Field LLM -- S2 Benchmark (55M params, 50M tokens)\n\nRuns both **Standard Transformer** and **SPECTRE-Wave V4.3.3** at S2 scale.\n\n| | S1 (verified) | S2 (this notebook) |\n|---|---|---|\n| Params | 22M | 55M |\n| Tokens | 20M | 50M |\n| Dataset | WikiText-2 | **WikiText-103** |\n| embed/layers/heads | 384/8/8 | 512/12/8 |\n| T4 time | ~25 min | ~2-3 hrs |\n| A100 time | ~12 min | ~55 min |\n\n**S1 results**: Wave PPL 229 vs Standard PPL 171 (1.34x gap on Colab T4)\n\n**Key changes for S2:**\n- Uses WikiText-103 (103M tokens) instead of WikiText-2 (2.6M) to avoid 19x data repetition\n- torch.compile disabled (incompatible with gradient checkpointing)\n- Bilinear gather dtype fix for fp16 AMP efficiency"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Setup\n",
    "!git clone https://github.com/Pankh-AI/wave-field-llm.git\n",
    "%cd wave-field-llm\n",
    "!pip install -q tokenizers datasets\n",
    "\n",
    "# Verify we have V4.3.3 code (NOT V4.3.4 which regressed)\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "from src import __version__\n",
    "print(f'\\nCode version: {__version__}')\n",
    "assert __version__ == '4.3.3', f'ERROR: Expected V4.3.3, got {__version__}! Do NOT run with V4.3.4+.'\n",
    "print('Version check PASSED')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: GPU check + VRAM estimation\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('No GPU! Go to Runtime > Change runtime type > T4 GPU')\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f'GPU: {gpu_name}')\n",
    "print(f'VRAM: {vram_gb:.1f} GB')\n",
    "\n",
    "# S2 VRAM estimate: ~55M model (110MB weights) + optimizer (440MB) + activations (~4GB at batch=12)\n",
    "# Total: ~5-6GB. T4 (16GB) and A100 (40/80GB) both have plenty of headroom.\n",
    "if vram_gb < 10:\n",
    "    print(f'WARNING: Only {vram_gb:.1f} GB VRAM. S2 needs ~6GB. May OOM at batch=12.')\n",
    "    print('Consider reducing batch size: os.environ[\"BATCH_SIZE\"] = \"8\"')\n",
    "else:\n",
    "    print(f'VRAM OK: {vram_gb:.1f} GB (S2 needs ~6GB)')\n",
    "\n",
    "# Time estimate\n",
    "if 'T4' in gpu_name:\n",
    "    print('\\nEstimated time: ~2-3 hours (both models)')\n",
    "    print('Standard: ~45-60 min, Wave: ~1.5-2 hrs')\n",
    "elif 'A100' in gpu_name:\n",
    "    print('\\nEstimated time: ~55 min (both models)')\n",
    "elif 'V100' in gpu_name:\n",
    "    print('\\nEstimated time: ~1.5-2 hours (both models)')\n",
    "else:\n",
    "    print(f'\\nUnknown GPU: {gpu_name}. Time varies.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 3: Run S2 benchmark (both Standard and Wave)\nimport os\nos.environ['SCALE'] = 'S2'              # S2 only (55M params, 50M tokens)\nos.environ['DATASET'] = '103'           # WikiText-103 (103M tokens, avoids 19x repetition)\nos.environ['MONITOR'] = '0'             # Skip monitor for speed on Colab\n# os.environ['DATASET'] = 'owt'         # Uncomment to use OpenWebText instead\n# os.environ['BATCH_SIZE'] = '8'        # Uncomment if OOM on low-VRAM GPUs\n# os.environ['MODEL'] = 'wave'          # Uncomment to run Wave only (skip Standard)\n\n!python benchmarks/benchmark_scaling.py",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 4: Results analysis\nimport json\n\nwith open('results/scaling_s2.json') as f:\n    data = json.load(f)\n\nprint('=' * 60)\nprint('  S2 BENCHMARK RESULTS (55M params, 50M tokens)')\nprint(f'  Dataset: {data[\"metadata\"].get(\"dataset\", \"unknown\")}')\nprint('=' * 60)\n\nresults = {}\nfor r in data['results']:\n    results[r['run_name']] = r\n    print(f\"\\n  {r['run_name']}\")\n    print(f\"    PPL:    {r['best_ppl']:.2f}\")\n    print(f\"    Acc:    {r['best_acc']:.2f}%\")\n    print(f\"    Params: {r['params']:,}\")\n    print(f\"    Speed:  {r['tokens_per_sec']:,} tok/s\")\n    print(f\"    Time:   {r['total_time_s']:.0f}s ({r['total_time_s']/60:.0f} min)\")\n    print(f\"    Epochs: {r.get('epochs', '?')}\")\n\n# Gap analysis\nwave_r = next((r for r in data['results'] if 'SPECTRE' in r['run_name']), None)\nstd_r = next((r for r in data['results'] if 'Standard' in r['run_name']), None)\nif wave_r and std_r:\n    std_ppl = std_r['best_ppl']\n    wave_ppl = wave_r['best_ppl']\n    gap = wave_ppl / std_ppl\n    print(f'\\n  GAP: {wave_ppl:.1f} / {std_ppl:.1f} = {gap:.2f}x')\n    print(f'  S1 gap was 1.34x (Colab T4) -- does scaling help?')\n    if gap < 1.34:\n        print(f'  YES! Gap narrowed from 1.34x to {gap:.2f}x at S2 scale.')\n    elif gap > 1.34:\n        print(f'  NO. Gap widened from 1.34x to {gap:.2f}x at S2 scale.')\n    else:\n        print(f'  Same gap at S2 scale.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Download results (optional)\n",
    "from google.colab import files\n",
    "files.download('results/scaling_s2.json')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}